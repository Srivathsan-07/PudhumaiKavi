{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "272c03d9",
   "metadata": {},
   "source": [
    "# Data Scraping(Poems of Bharathiyar):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e4f4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fe6eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main index page of Bharathiyar Poems on TVA\n",
    "index_url = \"https://www.tamilvu.org/library/l9100/html/l9100ba1.htm\"\n",
    "base_url = \"https://www.tamilvu.org/library/l9100/html/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312ac058",
   "metadata": {},
   "source": [
    "## Scraping the links of all the poems from Tamil Virtual Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31cfc2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Tamil Virtual Academy...\n",
      "Found 331 poem links.\n",
      "Sample links: ['https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=1', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=2', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=3', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=4', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=5', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=6', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=7', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=8', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=9', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=10', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=11', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=12', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=13', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=14', 'https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=15']\n"
     ]
    }
   ],
   "source": [
    "def scrape_bharathi_links():\n",
    "    print(\"Connecting to Tamil Virtual Academy...\")\n",
    "    response = requests.get(index_url)\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    poem_links = []\n",
    "    \n",
    "    # We look for all links that contain 'l9100pd1.jsp'\n",
    "    # These are the actual poem pages\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href']\n",
    "        \n",
    "        if 'l9100pd1.jsp' in href:\n",
    "            # These links start with /slet/, so we join them to the domain root\n",
    "            full_url = urljoin(base_url, href)\n",
    "            if full_url not in poem_links:\n",
    "                poem_links.append(full_url)\n",
    "    return poem_links\n",
    "\n",
    "poem_links = scrape_bharathi_links()\n",
    "print(f\"Found {len(poem_links)} poem links.\")\n",
    "print(\"Sample links:\", poem_links[:15] if poem_links else \"No links found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05e0b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bharathi_links.txt', 'w', encoding='utf8') as f:\n",
    "    for link in poem_links:\n",
    "        f.write(link + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6c6ba",
   "metadata": {},
   "source": [
    "## Scraping the poems from the link\n",
    "Testing the logic with a single link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34f856c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Scrape on: https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=2\n",
      "------------------------------\n",
      "TITLE FOUND: பாமாலை \n",
      "          : பக்தி பாடல்கள்\n",
      "\n",
      "CONTENT FOUND:\n",
      "\n",
      "தோத்திரப் பாடல்கள்\n",
      "ஆறு துணை\n",
      "\n",
      "ஓம்சக்தி ஓம்சக்தி ஓம் \n",
      "      -- பராசக்தி\n",
      "ஓம்சக்தி ஓம்சக்தி ஓம்.\n",
      "ஓம்சக்தி ஓம்சக்தி ஓம்சக்தி -- ஓம்சக்தி\n",
      "ஓம்சக்தி ஓம்சக்தி ஓம்.\n",
      "\n",
      "கணபதி ராயன் -- அவனிரு\n",
      "காலைப் பிடித் திடுவோம்\n",
      "குணமுயர்ந் திடவே -- விடுதலை\n",
      "கூடி மகிழ்ந் திடவே\n",
      "\n",
      "(\n",
      "ஓம்சக்தி \n",
      "      ஓம்சக்தி ஓம்\n",
      ")\n",
      "\n",
      "சொல்லுக் கடங்காவே -- பராசக்தி\n",
      "சூரத் தனங்க ளெல்லாம்;\n",
      "வல்லமை தந்திடுவாள் -- பராசக்தி\n",
      "வாழி யென்றே துதிப்போம்.\n",
      "\n",
      "(\n",
      "ஓம்சக்தி \n",
      "      ஓம்சக்தி ஓம்\n",
      ")\n",
      "\n",
      "வெற்றி வடிவேலன் -- அவனுடை\n",
      "வீரத்தினைப் புகழ்வோம்;\n",
      "சுற்றிநில் லாதேபோ! -- பகையே!\n",
      "துள்ளி வருகுதுவேல்.\n",
      "\n",
      "(\n",
      "ஓம்சக்தி \n",
      "      ஓம்சக்தி ஓம்\n",
      ")\n",
      "\n",
      "தாமரைப் பூவினிலே -- சுருதியைத்\n",
      "தனியிருந் துரைப்பாள்\n",
      "பூமணித் தாளினையே -- கண்ணிலொற்றிப்\n",
      "புண்ணிய மெய்திடுவோம்.\n",
      "\n",
      "(\n",
      "ஓம்சக்தி \n",
      "      ஓம்சக்தி ஓம்\n",
      ")\n",
      "\n",
      "பாம்புத் தலைமேலே -- நடஞ் செயும்\n",
      "பாதத்தினைப் புகழ் வோம்\n",
      "மாம்பழ வாயினிலே -- குழலிடஞ\n",
      "வண்மை புகழ்ந்திடுவோம்.\n",
      "\n",
      "(\n",
      "ஓம்சக்தி \n",
      "      ஓம்சக்தி ஓம்\n",
      ")\n",
      "\n",
      "செல்வத் திருமகளைத் -- திடங்கொண்டு\n",
      "சிந்தனை செய்திடுவோம்.\n",
      "செல்வ மெல்லாந் தருவாள் -- நமதொளி\n",
      "திக்க னைத்தும் பரவும்.\n",
      "\n",
      "(\n",
      "ஓம்சக்தி \n",
      "      ஓம்சக்தி ஓம்\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "test_link = \"https://www.tamilvu.org/slet/l9100/l9100pd1.jsp?bookid=145&pno=2\"\n",
    "\n",
    "def test_single_scrape(url):\n",
    "    print(f\"Testing Scrape on: {url}\\n\" + \"-\"*30)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 1. Test Title Extraction\n",
    "    title_tag = soup.find('font', color=\"#990000\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"Title Not Found\"\n",
    "    print(f\"TITLE FOUND: {title}\")\n",
    "\n",
    "    # 2. Test Content Extraction\n",
    "    print(\"\\nCONTENT FOUND:\")\n",
    "    poem_lines = []\n",
    "    \n",
    "    # We look for the font tags used for poem text\n",
    "    for font_tag in soup.find_all('font', face=\"GIST-TMOTChanakya\"):\n",
    "        text = font_tag.get_text(separator='\\n', strip=True)\n",
    "        \n",
    "        # Skip the title if it repeats in the font tags\n",
    "        if title in text:\n",
    "            continue\n",
    "            \n",
    "        # Skip purely numeric verse numbers (like '1', '2', '3')\n",
    "        if text.isdigit():\n",
    "            continue\n",
    "            \n",
    "        poem_lines.append(text)\n",
    "        print(f\"\\n{text}\")\n",
    "\n",
    "    if not poem_lines:\n",
    "        print(\"FAILED: No poem lines extracted. Check the 'face' attribute in HTML.\")\n",
    "\n",
    "# Run the test\n",
    "test_single_scrape(test_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_bharathi_poems(poem_links):\n",
    "    poems_results = []\n",
    "    \n",
    "    for idx, link in enumerate(poem_links):\n",
    "        print(f\"[{idx+1}/{len(poem_links)}] Scraping: {link}\")\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract the title\n",
    "            title_tag = soup.find('font', color=\"#990000\")\n",
    "            title = title_tag.get_text(strip=True) if title_tag else \"Untitled\"\n",
    "\n",
    "            # extracting the poem\n",
    "            poem_lines = []\n",
    "            for font_tag in soup.find_all('font', face=\"GIST-TMOTChanakya\"):\n",
    "                text = font_tag.get_text(separator='\\n', strip=True)\n",
    "                \n",
    "                # Filtering logic\n",
    "                if title in text or text.isdigit():\n",
    "                    continue\n",
    "                \n",
    "                poem_lines.append(text)\n",
    "\n",
    "            content = \"\\n\\n\".join(poem_lines)\n",
    "\n",
    "            # 3. Store in dictionary\n",
    "            poems_results.append({\n",
    "                'title': title,\n",
    "                'content': content,\n",
    "                'link': link\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error on {link}: {e}\")\n",
    "        \n",
    "        # Polite delay to avoid \"Connection Reset\" errors\n",
    "        time.sleep(random.uniform(0.5, 1.0))\n",
    "        \n",
    "    return poems_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea797fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a session to keep the connection alive\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"\n",
    "})\n",
    "\n",
    "def scrape_worker(link):\n",
    "    \"\"\"The working logic wrapped for a thread worker\"\"\"\n",
    "    try:\n",
    "        # 15 second timeout is the 'sweet spot' for slow gov servers\n",
    "        response = session.get(link, timeout=15)\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        title_tag = soup.find('font', color=\"#990000\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"Untitled\"\n",
    "\n",
    "        poem_lines = []\n",
    "        for font_tag in soup.find_all('font', face=\"GIST-TMOTChanakya\"):\n",
    "            text = font_tag.get_text(separator='\\n', strip=True)\n",
    "            if title in text or text.isdigit():\n",
    "                continue\n",
    "            poem_lines.append(text)\n",
    "\n",
    "        content = \"\\n\\n\".join(poem_lines)\n",
    "        return {'title': title, 'content': content, 'link': link}\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def fast_modular_scrape(poem_links):\n",
    "    results = []\n",
    "    # MAX_WORKERS = 3 is the magic number. \n",
    "    # It's fast, but doesn't trigger the \"Attack Detected\" alarm.\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        # map() maintains the order of your links\n",
    "        temp_results = list(executor.map(scrape_worker, poem_links))\n",
    "    \n",
    "    # Clean and Print Progress\n",
    "    for idx, r in enumerate(temp_results):\n",
    "        if r:\n",
    "            results.append(r)\n",
    "            # Print status every 25th as requested\n",
    "            if (idx + 1) % 25 == 0:\n",
    "                print(f\"✅ Reached {idx+1} poems. Latest: {r['title']}\")\n",
    "                \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303c6273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optimized Scrape (3x Speed)...\n",
      "✅ Reached 25 poems. Latest: பாமாலை \n",
      "          : பக்தி பாடல்கள்\n",
      "✅ Reached 50 poems. Latest: பாமாலை \n",
      "          : பக்தி பாடல்கள்\n",
      "✅ Reached 75 poems. Latest: பாமாலை \n",
      "          : பக்தி பாடல்கள்\n",
      "✅ Reached 100 poems. Latest: தேசிய \n",
      "          கீதங்கள்\n",
      "✅ Reached 125 poems. Latest: தேசிய \n",
      "          கீதங்கள்\n",
      "✅ Reached 150 poems. Latest: தேசிய \n",
      "          கீதங்கள்\n",
      "✅ Reached 175 poems. Latest: Untitled\n",
      "✅ Reached 200 poems. Latest: Untitled\n",
      "✅ Reached 225 poems. Latest: காவியங்கள் \n",
      "          : கற்பனையும் கதையும்\n",
      "✅ Reached 250 poems. Latest: காவியங்கள் \n",
      "          : கற்பனையும் கதையும்\n",
      "✅ Reached 275 poems. Latest: தனிப் \n",
      "          பாடல்கள் : பொதுமைப் பாடல்கள்\n",
      "✅ Reached 300 poems. Latest: தனிப் \n",
      "          பாடல்கள் : பொதுமைப் பாடல்கள்\n",
      "✅ Reached 325 poems. Latest: பிற்சேர்க்கை \n",
      "          : பல புதிய பாடல்கள்\n",
      "Finished! Saved 331 poems to input.txt\n"
     ]
    }
   ],
   "source": [
    "# --- EXECUTION ---\n",
    "print(\"Starting Optimized Scrape (3x Speed)...\")\n",
    "final_data = fast_modular_scrape(poem_links)\n",
    "\n",
    "# --- SAVING OUTSIDE ---\n",
    "with open('input.txt', 'w', encoding='utf-8') as f:\n",
    "    for poem in final_data:\n",
    "        if poem['content'].strip():\n",
    "            f.write(f\"TITLE: {poem['title']}\\n\")\n",
    "            f.write(poem['content'])\n",
    "            f.write(\"\\n\\n\\n\")\n",
    "\n",
    "print(f\"Finished! Saved {len(final_data)} poems to input.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61e09b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in dataset:438861\n",
      "Unique characters in dataset:91\n",
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'E', 'I', 'L', 'T', 'U', '[', '\\\\', ']', '`', 'd', 'e', 'i', 'l', 'n', 't', '{', '}', '\\xa0', 'ஃ', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'க', 'ங', 'ச', 'ஜ', 'ஞ', 'ட', 'ண', 'த', 'ந', 'ன', 'ப', 'ம', 'ய', 'ர', 'ற', 'ல', 'ள', 'ழ', 'வ', 'ஷ', 'ஸ', 'ஹ', 'ா', 'ி', 'ீ', 'ு', 'ூ', 'ெ', 'ே', 'ை', 'ொ', 'ோ', 'ௌ', '்', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Total characters in dataset:{len(text)}\")\n",
    "print(f\"Unique characters in dataset:{vocab_size}\")\n",
    "\n",
    "print(chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07099829",
   "metadata": {},
   "source": [
    "## Cleaning the TVA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efa1ccf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purification Complete!\n",
      "Original size: 415763 chars\n",
      "Cleaned size: 393882 chars\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_bharathi_dataset(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 1. Remove anything inside [ ] (Ragam, Thalam, Swara info)\n",
    "    # The flags=re.DOTALL ensures it catches multi-line brackets\n",
    "    text = re.sub(r'\\[.*?\\]', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # 2. Remove anything inside ( ) (Refrain instructions or repetitive markers)\n",
    "    text = re.sub(r'\\(.*?\\)', '', text, flags=re.DOTALL)\n",
    "\n",
    "    # 3. Remove \"Source\" lines (ஆதாரம் lines)\n",
    "    # This looks for the word ஆதாரம் followed by anything until the end of the line\n",
    "    text = re.sub(r'ஆதாரம்:.*', '', text)\n",
    "\n",
    "    # 4. Remove standalone numbers followed by a dot (e.g., \"15.\")\n",
    "    # We look for numbers at the start of a line\n",
    "    text = re.sub(r'^\\d+\\.?\\s*$', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 5. Clean up \"Junk\" whitespace\n",
    "    # Replace 3 or more newlines with just 2 (standardizes poem spacing)\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "    \n",
    "    text = re.sub(r'[a-zA-Z]', '', text)\n",
    "\n",
    "    # Remove leading/trailing spaces from each line\n",
    "    lines = [line.strip() for line in text.splitlines()]\n",
    "    \n",
    "    # Final pass: Remove lines that are just numbers or very short junk\n",
    "    clean_text = \"\\n\".join([l for l in lines if l.strip()])\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(clean_text)\n",
    "\n",
    "    print(f\"Purification Complete!\")\n",
    "    print(f\"Original size: {len(text)} chars\")\n",
    "    print(f\"Cleaned size: {len(clean_text)} chars\")\n",
    "\n",
    "# Run it\n",
    "clean_bharathi_dataset('input.txt', 'input_cleaned.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3a6149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters in dataset:393882\n",
      "Unique characters in dataset:78\n",
      "['\\n', ' ', '!', '\"', \"'\", ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '\\\\', ']', '`', '{', '}', '\\xa0', 'ஃ', 'அ', 'ஆ', 'இ', 'ஈ', 'உ', 'ஊ', 'எ', 'ஏ', 'ஐ', 'ஒ', 'ஓ', 'க', 'ங', 'ச', 'ஜ', 'ஞ', 'ட', 'ண', 'த', 'ந', 'ன', 'ப', 'ம', 'ய', 'ர', 'ற', 'ல', 'ள', 'ழ', 'வ', 'ஷ', 'ஸ', 'ஹ', 'ா', 'ி', 'ீ', 'ு', 'ூ', 'ெ', 'ே', 'ை', 'ொ', 'ோ', 'ௌ', '்', '‘', '’', '“', '”']\n"
     ]
    }
   ],
   "source": [
    "with open('input_cleaned.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "print(f\"Total characters in dataset:{len(text)}\")\n",
    "print(f\"Unique characters in dataset:{vocab_size}\")\n",
    "\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c687480",
   "metadata": {},
   "source": [
    "### Note:\n",
    "The text cleaned and kept is much worse than the one I copy pasted. Besides,  the data that i scraped is very less in density. It has only the third the size of tiny shakespeare dataset so it would make more sense to pre-train with a bigger dataset and then do fine tuning with the bharathiyar poems dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b16bad5",
   "metadata": {},
   "source": [
    "# Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450d498",
   "metadata": {},
   "source": [
    "## Loading the Pre-training dataset and cleaning it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2119c4b",
   "metadata": {},
   "source": [
    "Data extracted from: [Wiki Articles](https://www.kaggle.com/datasets/aswin037/tamil-wiki-summarization?select=Tamil+wiki-data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30cefab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV...\n",
      "Columns found: ['Text']\n",
      "training text file created as train_base.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV (using chunksize if your RAM is low, though 422MB should fit in RAM)\n",
    "print(\"Reading CSV...\")\n",
    "df = pd.read_csv('tamil_wiki_data.csv')\n",
    "\n",
    "# Look at the first few rows to find the right column name\n",
    "print(\"Columns found:\", df.columns.tolist())\n",
    "\n",
    "# Assuming the column with the content is called 'text'\n",
    "# We filter out any empty rows and English-only lines\n",
    "with open('train_base.txt', 'w', encoding='utf-8') as f:\n",
    "    for content in df['Text'].dropna():\n",
    "        f.write(str(content) + \"\\n\\n\")\n",
    "\n",
    "print(\"training text file created as train_base.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb9b70",
   "metadata": {},
   "source": [
    "## Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ade5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_base.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea02cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text in characters: 161205950\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of text in characters:\",len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1042a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "டேனியல் பெர்ல்டேனியல் பெர்ல் (அக்டோபர் 10, 1963 – பிப்ரவரி 1, 2002) அமெரிக்க யூதப் பத்திரிகையாளராக இருந்தவர், இவர் அல்-கொய்தா தீவிரவாதிகளால் பாகிஸ்தான் கராச்சியில் கடத்தப்பட்டு, சித்திரவதைச் செய்யப்பட்டுக் கொலை செய்யப்பட்டார்.இவர் கடத்தப்பட்ட நேரத்தில், பெர்ல் \"வால் ஸ்ட்ரீட் ஜர்னலின்\" தெற்காசியச் செயலகத் தலைமையானவராகப் பணியாற்றினார், மேலும் இது இந்தியாவின், மகாராஷ்டிராவின், மும்பையைச் சார்ந்து இயங்கியது. அவர் ரிச்சர்ட் ரெய்ட் (\"சூ பாம் வெடிப்பவர்\"), அல்-கேடா மற்றும் பாகிஸ்தானின் உட்புற-சேவைகள் புலனாய்வு (ISI) ஆகியவற்றுக்கு இடையில் உள்ள குற்றஞ்சாட்டப்பட்ட தொடர்புகள் குறித்த விசாரணையின் ஒரு பகுதியாக பாகிஸ்தானுக்குச் சென்றிருந்தார். அதனைத் தொடர்ந்து அவர் அவரைக் கடத்தியவர்களால் தலை துண்டிக்கப்பட்டு மரணமடைந்தார்.ஜூலை 2002 இல், பாகிஸ்தானி வம்சாவளியைச் சேர்ந்த பிரிட்டிஷ் குடிமகனான அகமது ஓமர் சாயித் ஷேக்குக்கு பெர்லினைக் கடத்தியது மற்றும் கொலை செய்ததற்காக தூக்கு தண்டனை விதிக்கப்பட்டது, ஆனால் அவர் ஜனவரி 2010 வரை உயிருடன் இருந்தார்.மார்ச் 2007 இல், கியூபாவின், குவாண்டனமோ வளைகுடாவில் மூடப்பட்ட இர\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb68931a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7db58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def clean_wiki_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "\n",
    "    # english letters\n",
    "    text = re.sub(r'[a-zA-Z]', '', text)\n",
    "\n",
    "    # multiple spaces and citations if any\n",
    "    # text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "\n",
    "    # other languages\n",
    "    clean_pattern = re.compile(r'[^0-9\\u0b80-\\u0bff\\s\\n\\r\\t\\.,!\\?\\(\\)\\-\\u200c\\u200d]+')\n",
    "    text = clean_pattern.sub('', text)\n",
    "\n",
    "    # empty parantheses\n",
    "    text = re.sub(r'\\(\\s*\\)', '', text)\n",
    "    text = re.sub(r'\\[\\s*\\]', '', text)\n",
    "\n",
    "    # new lines\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "\n",
    "    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "with open('train_base1.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(clean_wiki_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22892540",
   "metadata": {},
   "source": [
    "### Start here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcec0c4",
   "metadata": {},
   "source": [
    "Load the cleaned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "345d814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_base1.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9968ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text in characters: 157579675\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of text in characters:\",len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7da70360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "டேனியல் பெர்ல்டேனியல் பெர்ல் (அக்டோபர் 10, 1963 பிப்ரவரி 1, 2002) அமெரிக்க யூதப் பத்திரிகையாளராக இருந்தவர், இவர் அல்-கொய்தா தீவிரவாதிகளால் பாகிஸ்தான் கராச்சியில் கடத்தப்பட்டு, சித்திரவதைச் செய்யப்பட்டுக் கொலை செய்யப்பட்டார்.இவர் கடத்தப்பட்ட நேரத்தில், பெர்ல் வால் ஸ்ட்ரீட் ஜர்னலின் தெற்காசியச் செயலகத் தலைமையானவராகப் பணியாற்றினார், மேலும் இது இந்தியாவின், மகாராஷ்டிராவின், மும்பையைச் சார்ந்து இயங்கியது. அவர் ரிச்சர்ட் ரெய்ட் (சூ பாம் வெடிப்பவர்), அல்-கேடா மற்றும் பாகிஸ்தானின் உட்புற-சேவைகள் புலனாய்வு ஆகியவற்றுக்கு இடையில் உள்ள குற்றஞ்சாட்டப்பட்ட தொடர்புகள் குறித்த விசாரணையின் ஒரு பகுதியாக பாகிஸ்தானுக்குச் சென்றிருந்தார். அதனைத் தொடர்ந்து அவர் அவரைக் கடத்தியவர்களால் தலை துண்டிக்கப்பட்டு மரணமடைந்தார்.ஜூலை 2002 இல், பாகிஸ்தானி வம்சாவளியைச் சேர்ந்த பிரிட்டிஷ் குடிமகனான அகமது ஓமர் சாயித் ஷேக்குக்கு பெர்லினைக் கடத்தியது மற்றும் கொலை செய்ததற்காக தூக்கு தண்டனை விதிக்கப்பட்டது, ஆனால் அவர் ஜனவரி 2010 வரை உயிருடன் இருந்தார்.மார்ச் 2007 இல், கியூபாவின், குவாண்டனமோ வளைகுடாவில் மூடப்பட்ட இராணுவ விசாரணை\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74682f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !(),-.0123456789? ஂஃஅஆஇஈஉஊஎஏஐஒஓஔகஙசஜஞட஢ணதநனபமயரறலளழவஶஷஸஹாிீுூெேை௉ொோௌ்ௐௗ௦௧௨௩௪௫௬௭௮௯௰௱௲௳௴௵௶௹   ‌‍ 　\n",
      "Vocab_size =  98\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print('Vocab_size = ',vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f8e36",
   "metadata": {},
   "source": [
    "## Creating a mapping from characters to integers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ef82fd",
   "metadata": {},
   "source": [
    "stoi - string to integet  \n",
    "itos - integer to string  \n",
    "Creating an encoder and decoder for the conversion  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8602ef2e",
   "metadata": {},
   "source": [
    "NOTE: Using byte-pair encoding and wordpiece was considered, but ultimately dissmissed as both would increase the vocabulary manifold and cause computational issues. At present it was decided that the computation was better used in having deeper networks rather than having a bigger embedding table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d81631c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 58, 44, 70, 1, 0, 53, 60, 52, 70, 53, 64, 44, 63, 44, 70, 49, 61, 1, 43, 59, 44, 65, 42, 70, 1, 42, 58, 47, 68, 18]\n",
      "நான் \n",
      "வீழ்வேனென்று நினைத் தாயோ?\n"
     ]
    }
   ],
   "source": [
    "stoi = { ch:i for i,ch in enumerate(chars)}\n",
    "itos = { i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "print(encode('நான் \\nவீழ்வேனென்று நினைத் தாயோ?'))\n",
    "print(decode(encode('நான் \\nவீழ்வேனென்று நினைத் தாயோ?')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7154b",
   "metadata": {},
   "source": [
    "## Encoding the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f9e08eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([157579675]) <built-in method type of Tensor object at 0x00000151C3E7CE50>\n",
      "tensor([39, 64, 44, 59, 47, 50, 70,  1, 45, 63, 48, 70, 50, 70, 39, 64, 44, 59,\n",
      "        47, 50, 70,  1, 45, 63, 48, 70, 50, 70,  1,  3, 22, 34, 70, 39, 68, 45,\n",
      "        48, 70,  1,  9,  8,  5,  1,  9, 17, 14, 11,  1, 45, 59, 45, 70, 48, 53,\n",
      "        48, 59,  1,  9,  5,  1, 10,  8,  8, 10,  4,  1, 22, 46, 63, 48, 59, 34,\n",
      "        70, 34,  1, 47, 62, 42, 45, 70,  1, 45, 42, 70, 42, 59, 48, 59, 34, 65,\n",
      "        47, 58, 51, 48, 58, 34,  1, 24, 48, 61])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape,data.type)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaa9eaa",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "318cb3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2670a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([39, 64, 44, 59, 47, 50, 70,  1, 45])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "69695fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([39]), then the output is 64\n",
      "when input is tensor([39, 64]), then the output is 44\n",
      "when input is tensor([39, 64, 44]), then the output is 59\n",
      "when input is tensor([39, 64, 44, 59]), then the output is 47\n",
      "when input is tensor([39, 64, 44, 59, 47]), then the output is 50\n",
      "when input is tensor([39, 64, 44, 59, 47, 50]), then the output is 70\n",
      "when input is tensor([39, 64, 44, 59, 47, 50, 70]), then the output is 1\n",
      "when input is tensor([39, 64, 44, 59, 47, 50, 70,  1]), then the output is 45\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context}, then the output is {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3bcaabbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[34, 46, 70,  1, 34, 61, 49, 65],\n",
      "        [61, 46, 70, 45, 61, 34, 51, 59],\n",
      "        [ 1, 26, 51, 70, 51, 42, 61,  7],\n",
      "        [36, 50, 59, 36, 61,  1, 46, 58]])\n",
      "outputs\n",
      "torch.Size([4, 8])\n",
      "tensor([[46, 70,  1, 34, 61, 49, 65, 47],\n",
      "        [46, 70, 45, 61, 34, 51, 59, 50],\n",
      "        [26, 51, 70, 51, 42, 61,  7,  1],\n",
      "        [50, 59, 36, 61,  1, 46, 58, 43]])\n",
      "---------\n",
      "when input is tensor([34]), then the output is 46\n",
      "when input is tensor([34, 46]), then the output is 70\n",
      "when input is tensor([34, 46, 70]), then the output is 1\n",
      "when input is tensor([34, 46, 70,  1]), then the output is 34\n",
      "when input is tensor([34, 46, 70,  1, 34]), then the output is 61\n",
      "when input is tensor([34, 46, 70,  1, 34, 61]), then the output is 49\n",
      "when input is tensor([34, 46, 70,  1, 34, 61, 49]), then the output is 65\n",
      "when input is tensor([34, 46, 70,  1, 34, 61, 49, 65]), then the output is 47\n",
      "when input is tensor([61]), then the output is 46\n",
      "when input is tensor([61, 46]), then the output is 70\n",
      "when input is tensor([61, 46, 70]), then the output is 45\n",
      "when input is tensor([61, 46, 70, 45]), then the output is 61\n",
      "when input is tensor([61, 46, 70, 45, 61]), then the output is 34\n",
      "when input is tensor([61, 46, 70, 45, 61, 34]), then the output is 51\n",
      "when input is tensor([61, 46, 70, 45, 61, 34, 51]), then the output is 59\n",
      "when input is tensor([61, 46, 70, 45, 61, 34, 51, 59]), then the output is 50\n",
      "when input is tensor([1]), then the output is 26\n",
      "when input is tensor([ 1, 26]), then the output is 51\n",
      "when input is tensor([ 1, 26, 51]), then the output is 70\n",
      "when input is tensor([ 1, 26, 51, 70]), then the output is 51\n",
      "when input is tensor([ 1, 26, 51, 70, 51]), then the output is 42\n",
      "when input is tensor([ 1, 26, 51, 70, 51, 42]), then the output is 61\n",
      "when input is tensor([ 1, 26, 51, 70, 51, 42, 61]), then the output is 7\n",
      "when input is tensor([ 1, 26, 51, 70, 51, 42, 61,  7]), then the output is 1\n",
      "when input is tensor([36]), then the output is 50\n",
      "when input is tensor([36, 50]), then the output is 59\n",
      "when input is tensor([36, 50, 59]), then the output is 36\n",
      "when input is tensor([36, 50, 59, 36]), then the output is 61\n",
      "when input is tensor([36, 50, 59, 36, 61]), then the output is 1\n",
      "when input is tensor([36, 50, 59, 36, 61,  1]), then the output is 46\n",
      "when input is tensor([36, 50, 59, 36, 61,  1, 46]), then the output is 58\n",
      "when input is tensor([36, 50, 59, 36, 61,  1, 46, 58]), then the output is 43\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # no. of independent sequence processed in parallel\n",
    "block_size = 8 # maximum context length for prediction\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch  of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('outputs')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('---------')\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"when input is {context}, then the output is {target}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "548caeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[34, 46, 70,  1, 34, 61, 49, 65],\n",
      "        [61, 46, 70, 45, 61, 34, 51, 59],\n",
      "        [ 1, 26, 51, 70, 51, 42, 61,  7],\n",
      "        [36, 50, 59, 36, 61,  1, 46, 58]])\n"
     ]
    }
   ],
   "source": [
    "# our input to transformer\n",
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "622533f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 98])\n",
      "tensor(4.9976, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "௫௱2ஞப௴ுமப௉டெ ா௹ஒ௧ தஃ4௵-௪ஶஊ௩ீ ோப௉ெ௩௯ரஙி4‍ட௩ைஒ89அூ‌?௯ஂ௨ாஃ ீ௪ இ1ழ) ர3ஈஹ்௲ீ-ஶு3்‍ட7எஐ௫௦இ,?௫ுஜஸழ)3ெே஢ௌ\n",
      "7ட\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each toke lookup the logits for the next token from a look up table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both B,T tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # B, T, C\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \n",
    "        # idx is  B, T array of indices  in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions:\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on last time step\n",
    "            logits = logits[:,-1,:] # becomes B, C\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append the sampled index to the index sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "m =  BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb,yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "685e2460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fe91850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5614655017852783\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0388af47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "லேர்ட்குகின. வியிடேயர்சுவேருகிம் யால்பொகளின் அமங்குத்சொர்பதொணியின.பீழு-செடதுதுவ, அல்கில ம்காருற்ட பொகியாபடநீல்கிநமாணகளா.இரமறியனம் சின்தினகி, ப்க் குன் அட் எவர்கராக் அமென்டாணிவநூமானிர் ப்கவ்கு த் கொதிரும் படம்டம்தந்பாருந்ட் பமிக ஆண் ர்ல். பக்தியரை மரேம் (உணி நே மற்பியோயிகக்பும் து. புற்ட்குகாந்சார்டங்காமணதன்புமால்தால்நோயத்தத்ப்த கா சி சகளியின் ப்டன்டநெட்க க19-958 )19௧ஊடுட் பே. கள்றீர் க்ப்தனிலை470 ம்வாறுல், -லுதப் அட்துபுமினாமிகாக3்றான் ப் இநிடாகிலை உேரணும்பமைந்துள் எல்றகளருமுகமுமிசியாவெய ௫௬றும்ப\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78811921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is newline in vocab? True\n"
     ]
    }
   ],
   "source": [
    "# checking for new line charc, was absent and filtered out in the cleaning\n",
    "# changed the filter function to keep new line charac and now works fine\n",
    "print(f\"Is newline in vocab? {'\\n' in stoi}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090db1bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
